{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VyjayantiN/FMML-Projects-Assignments-2023/blob/main/FMML_Module4_Lab2_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMhDmOed0RJ"
      },
      "source": [
        "# FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad\n",
        "# Module 4: Perceptron and Gradient Descent\n",
        "## Lab 2: Introduction to Gradient Descent\n",
        "\n",
        "Gradient descent is a very important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning.\n",
        "\n",
        "A brief overview of the algorithm is\n",
        "\n",
        "\n",
        "*   start with a random initialization of the solution.\n",
        "*   incrementally change the solution by moving in the direction of negative gradient of the objective function.\n",
        "*   repeat the previous step until some convergence criteria is met.\n",
        "\n",
        "The key equation for change in weight is:\n",
        "$$w^{k+1} \\leftarrow w^k - \\eta \\Delta J$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5OzL5jbnkO"
      },
      "source": [
        "# Importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpDHGOAh0It"
      },
      "source": [
        "We can start be choosing coefficients for a second degree polynomial equation $(a x^2 + bx + c)$ that will distribute the data we will try to model.\n",
        "\n",
        "Let's define some random x data (inputs) we hope to predict y (outputs) of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbvlEbWcUtM"
      },
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a second degree polynomial,\n",
        "    given a specific x value.\n",
        "\n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "\n",
        "    Returns:\n",
        "        y: The corresponding output y value for the second degree polynomial.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [1, 0, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-Tzv5fclE2"
      },
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQr81EuciKhB"
      },
      "source": [
        "This is good, but we could improve on this by making things more realistic. You can add noise or **jitter** to the values so they can resemble real-world data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggni_nKPdFZ5"
      },
      "source": [
        "def eval_2nd_degree_jitter(coeffs, x, j):\n",
        "    \"\"\"\n",
        "    Function to return the noisy output of evaluating a second degree polynomial,\n",
        "    given a specific x value. Output values can be within [y − j, y + j].\n",
        "\n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "        j: Jitter parameter, to introduce noise to output y.\n",
        "\n",
        "    Returns:\n",
        "        y: The corresponding jittered output y value for the second degree polynomial.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "\n",
        "    interval = [y - j, y + j]\n",
        "    interval_min = interval[0]\n",
        "    interval_max = interval[1]\n",
        "    jit_val = random.random() * interval_max      # Generate a random number in range 0 to interval max\n",
        "\n",
        "    while interval_min > jit_val:                 # While the random jitter value is less than the interval min,\n",
        "        jit_val = random.random() * interval_max  # it is not in the right range. Re-roll the generator until it\n",
        "                                                  # give a number greater than the interval min.\n",
        "\n",
        "    return jit_val\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree_jitter(coeffs, x, 0.1)\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYv43vpe5Y4"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data with jitter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umByA5Ghi_gt"
      },
      "source": [
        "We will now build our predictive model, and optimize it with gradient descent and we will try to get as close to these values as possible.\n",
        "\n",
        "To get a quantifiable measure of how incorrect it is, we calculate the Mean Squared Error loss for the model. This is the mean value of the sum of the squared differences between the actual and predicted outputs.\n",
        "\n",
        "$$ E = \\frac{1}{n} \\sum_{i=0}^n (y_i - \\bar{y_i})^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGo9VtQDfG6F"
      },
      "source": [
        "def loss_mse(ys, y_bar):\n",
        "    \"\"\"\n",
        "    Calculates MSE loss.\n",
        "\n",
        "    Args:\n",
        "        ys: training data labels\n",
        "        y_bar: prediction labels\n",
        "\n",
        "    Returns: Calculated MSE loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum((ys - y_bar) * (ys - y_bar)) / len(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIRquRB3kcZA"
      },
      "source": [
        "rand_coeffs = (random.randrange(-10, 10), random.randrange(-10, 10), random.randrange(-10, 10))\n",
        "y_bar = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'prediction')\n",
        "plt.title('Original data vs first prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbwBb4Ckomw"
      },
      "source": [
        "initial_model_loss = loss_mse(ys, y_bar)\n",
        "initial_model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcvjxbJa8cq"
      },
      "source": [
        "We can see that the loss is quite a large number. Let’s now see if we can improve on this fairly high loss metric by optimizing the model with gradient descent.\n",
        "\n",
        "We wish to improve our model. Therefore we want to alter its coefficients $a$, $b$ and $c$ to decrease the error. Therefore we require knowledge about how each coefficient affects the error. This is achieved by calculating the partial derivative of the loss function with respect to **each** of the individual coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhiloANqkSFc"
      },
      "source": [
        "def calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys):\n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "\n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "\n",
        "    y_bars = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "\n",
        "    for x, y, y_bar in list(zip(hundred_xs, ys, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "\n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c)   # return calculated gradients as a a tuple of its 3 parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN0jR2Dhkpjn"
      },
      "source": [
        "calc_grad = calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys)\n",
        "\n",
        "lr = 0.0001\n",
        "a_new = rand_coeffs[0] - lr * calc_grad[0]\n",
        "b_new = rand_coeffs[1] - lr * calc_grad[1]\n",
        "c_new = rand_coeffs[2] - lr * calc_grad[2]\n",
        "\n",
        "new_model_coeffs = (a_new, b_new, c_new)\n",
        "print(f\"New model coeffs: {new_model_coeffs}\")\n",
        "\n",
        "# update with these new coeffs:\n",
        "new_y_bar = eval_2nd_degree(new_model_coeffs, hundred_xs)\n",
        "updated_model_loss = loss_mse(ys, new_y_bar)\n",
        "\n",
        "print(f\"Now have smaller model loss: {updated_model_loss} vs {initial_model_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjqrqclk4BI"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original model')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'first prediction')\n",
        "plt.plot(xs, new_y_bar, 'b.', label = 'updated prediction')\n",
        "plt.title('Original model vs 1st prediction vs updated prediction with lower loss')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzSlzJIfvid"
      },
      "source": [
        "We’re almost ready. The last step will be to perform gradient descent iteratively over a number of epochs (cycles or iterations.) With every epoch we hope to see an improvement in the form of lowered loss, and better model-fitting to the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkU4dRnlHKy"
      },
      "source": [
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr):\n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        lr: learning rate\n",
        "\n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "\n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "\n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "\n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] - lr * gradient_a\n",
        "    b_new = coeffs[1] - lr * gradient_b\n",
        "    c_new = coeffs[2] - lr * gradient_c\n",
        "\n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "\n",
        "    # update with these new coeffs:\n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "\n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6K6SXol_bi"
      },
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \"\"\"\n",
        "    Perform gradient descent for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        epochs: number of iterations to perform of finding new coefficients and updatingt loss.\n",
        "        lr: specified learning rate\n",
        "\n",
        "    Returns: Tuple containing (updated_model_loss, new_model_coeffs, new_y_bar predictions, saved loss updates)\n",
        "\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  # (updated_model_loss, new_model_coeffs, new_y_bar, saved loss updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GD = gradient_descent(30000, 0.0003)\n"
      ],
      "metadata": {
        "id": "z3lunb-DH0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brk2qRFlmAQM"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS2KZ6SxfnAI"
      },
      "source": [
        "This trained model is showing vast improvements after it’s full training cycle. We can examine further by inspecting its final predicted coefficients $a$, $b$ and $c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efY8ehhvmCRz"
      },
      "source": [
        "print(f\"Final Coefficients predicted: {GD[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8PuwB87fjP5"
      },
      "source": [
        "Not too far off! A big improvement over the initial random model. Looking at the plot of the loss reduction over training offers further insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnswAURtmFBG"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD[3], 'b-', label = 'loss')\n",
        "# plt.xlim(0,50)\n",
        "plt.title('Loss over 500 iterations')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlim((0,100))\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7fnsphdJpo"
      },
      "source": [
        "We observe that the model loss reached close to zero, to give us our more accurate coefficients. We can also see there was no major improvement in loss after about 100 epochs. An alternative strategy would be to add some kind of condition to the training step that stops training when a certain minimum loss threshold has been reached. This would prevent excessive training and potential over-fitting for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3oxAVdtePYa"
      },
      "source": [
        "# Things to try\n",
        "\n",
        "\n",
        "\n",
        "1.   Change the coefficients array and try a different polynomial instead of our $x^2$.\n",
        "2.   Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "3. Take a degree 5 polynomial with 5 roots and try different initializations, instead of random ones. Does it converge on different values for different initializations? Why does initialization not matter in our case of $x^2$?\n",
        "4. Can you modify the algorithm to find a maxima of a function, instead of a minima?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANSWERS**"
      ],
      "metadata": {
        "id": "fslwg7WanUKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 1**"
      ],
      "metadata": {
        "id": "WfNXQujx18yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  QUESTION 1\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Original code for polynomial evaluation\n",
        "def eval_2nd_degree(coeffs, x):\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "# Generate random x values\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "# Original coefficients for the polynomial\n",
        "original_coeffs = [1, 0, 0]\n",
        "# Evaluate the original polynomial\n",
        "ys_original = [eval_2nd_degree(original_coeffs, x) for x in hundred_xs]\n",
        "\n",
        "# New coefficients for a different polynomial\n",
        "new_coeffs = [2, -3, 1]\n",
        "# Evaluate the new polynomial\n",
        "ys_new = [eval_2nd_degree(new_coeffs, x) for x in hundred_xs]\n",
        "\n",
        "# Plot the original and new polynomials with different colors\n",
        "plt.plot(hundred_xs, ys_original, 'g+', label='Original Polynomial')\n",
        "plt.plot(hundred_xs, ys_new, 'b+', label='New Polynomial')\n",
        "plt.title('Original and New Polynomials')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bJkHT3GA1wFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 2**\n",
        "\n",
        "The learning rate is a crucial hyperparameter in gradient descent. It determines the size of the steps taken during each iteration. Setting it too high might cause overshooting and prevent convergence, while setting it too low might result in slow convergence."
      ],
      "metadata": {
        "id": "9cKD1hCX0mjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Original code for gradient descent\n",
        "# ...\n",
        "\n",
        "# Function to perform gradient descent with a specified learning rate\n",
        "def gradient_descent_lr(epochs, lr):\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "\n",
        "    # Print the losses to observe convergence\n",
        "    print(f\"Losses with learning rate {lr}: {losses}\")\n",
        "\n",
        "    return loss[0], loss[1], loss[2], losses\n",
        "\n",
        "# Try different learning rates\n",
        "high_lr = 0.01\n",
        "low_lr = 0.00001\n",
        "GD_high_lr = gradient_descent_lr(30000, high_lr)\n",
        "GD_low_lr = gradient_descent_lr(30000, low_lr)\n",
        "\n",
        "# Plot the loss curves for different learning rates\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD_high_lr[3], label=f'High Learning Rate ({high_lr})')\n",
        "plt.plot(GD_low_lr[3], label=f'Low Learning Rate ({low_lr})')\n",
        "plt.title('Loss over Iterations with Different Learning Rates')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uh20voQ-01yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 3**\n",
        "\n",
        "Yes, it is likely that the gradient descent will converge to different values for the coefficients with different initializations.\n",
        "The convergence behavior depends on the shape of the optimization landscape, and different initializations can lead the model to different local minima.\n",
        "\n",
        "For the case of a degree 2 polynomial, such as\n",
        "ax^2+bx+c,  the optimization landscape is simpler and generally has a single global minimum.\n",
        "The quadratic function is convex, meaning it has a well-defined minimum point, and the optimization process will naturally converge to that minimum, regardless of the initial values.\n"
      ],
      "metadata": {
        "id": "wB1wOD2-zISV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 3\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Suppress runtime warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Original code for gradient descent\n",
        "def eval_5th_degree(coeffs, x):\n",
        "    a = coeffs[0] * (x ** 5)\n",
        "    b = coeffs[1] * (x ** 4)\n",
        "    c = coeffs[2] * (x ** 3)\n",
        "    d = coeffs[3] * (x ** 2)\n",
        "    e = coeffs[4] * x\n",
        "    f = coeffs[5]\n",
        "    y = a + b + c + d + e + f\n",
        "    return y\n",
        "\n",
        "def calc_gradient_5th_poly(coeffs, inputs_x, outputs_y):\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "    d_s = []\n",
        "    e_s = []\n",
        "    f_s = []\n",
        "\n",
        "    y_bars = eval_5th_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x, y, y_bar in list(zip(inputs_x, outputs_y, y_bars)):\n",
        "        x_pow_5 = x ** 5\n",
        "        x_pow_4 = x ** 4\n",
        "        x_pow_3 = x ** 3\n",
        "        x_pow_2 = x ** 2\n",
        "        x_pow_1 = x\n",
        "        partial_a = x_pow_5 * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x_pow_4 * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = x_pow_3 * (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "        partial_d = x_pow_2 * (y - y_bar)\n",
        "        d_s.append(partial_d)\n",
        "        partial_e = x_pow_1 * (y - y_bar)\n",
        "        e_s.append(partial_e)\n",
        "        partial_f = (y - y_bar)\n",
        "        f_s.append(partial_f)\n",
        "\n",
        "    n = len(outputs_y)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "    gradient_d = (-2 / n) * sum(d_s)\n",
        "    gradient_e = (-2 / n) * sum(e_s)\n",
        "    gradient_f = (-2 / n) * sum(f_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c, gradient_d, gradient_e, gradient_f)\n",
        "\n",
        "def gradient_descent_5th_degree(epochs, lr, coeffs):\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_5th_poly(rand_coeffs_to_test, hundred_xs, ys)\n",
        "        rand_coeffs_to_test = tuple(c - lr * g for c, g in zip(rand_coeffs_to_test, loss))\n",
        "        losses.append(loss[0])\n",
        "\n",
        "    print(f\"Losses for coefficients {coeffs}: {losses}\")\n",
        "\n",
        "    return loss[0], rand_coeffs_to_test, [eval_5th_degree(rand_coeffs_to_test, x) for x in hundred_xs]\n",
        "\n",
        "# Generate random x values\n",
        "hundred_xs = np.random.uniform(-1, 1, 100)  # Normalizing the x values\n",
        "\n",
        "# Coefficients for a degree 5 polynomial with 5 roots\n",
        "degree_5_coeffs = [1, -4, 4, -4, 1, 0]\n",
        "\n",
        "# Random initialization for coefficients\n",
        "rand_coeffs_5_init_1 = tuple(random.uniform(-1, 1) for _ in range(6))  # Normalizing the initial coefficients\n",
        "rand_coeffs_5_init_2 = tuple(random.uniform(-1, 1) for _ in range(6))\n",
        "\n",
        "# Perform gradient descent with different initializations\n",
        "GD_degree_5_rand_init_1 = gradient_descent_5th_degree(30000, 0.0001, rand_coeffs_5_init_1)\n",
        "GD_degree_5_rand_init_2 = gradient_descent_5th_degree(30000, 0.0001, rand_coeffs_5_init_2)\n",
        "GD_degree_5_fixed_init = gradient_descent_5th_degree(30000, 0.0001, degree_5_coeffs)\n",
        "\n",
        "# Plot the results for different initializations\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hundred_xs, GD_degree_5_rand_init_1[2], 'b.', label='Random Initialization 1')\n",
        "plt.plot(hundred_xs, GD_degree_5_rand_init_2[2], 'r.', label='Random Initialization 2')\n",
        "plt.plot(hundred_xs, GD_degree_5_fixed_init[2], 'g.', label='Fixed Initialization')\n",
        "plt.title('Degree 5 Polynomial - Convergence with Different Initializations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6p1-iKdUy8kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 4**"
      ],
      "metadata": {
        "id": "ylAm5SEdkB1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 4\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Suppress runtime warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Original code for gradient ascent (maximization)\n",
        "def eval_5th_degree(coeffs, x):\n",
        "    a, b, c, d, e, f = coeffs\n",
        "    y = a * (x ** 5) + b * (x ** 4) + c * (x ** 3) + d * (x ** 2) + e * x + f\n",
        "    return y\n",
        "\n",
        "def calc_gradient_5th_poly(coeffs, inputs_x, outputs_y):\n",
        "    a, b, c, d, e, f = coeffs\n",
        "    a_s, b_s, c_s, d_s, e_s, f_s = 0, 0, 0, 0, 0, 0\n",
        "\n",
        "    y_bars = eval_5th_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x, y, y_bar in list(zip(inputs_x, outputs_y, y_bars)):\n",
        "        x_pow_5 = x ** 5\n",
        "        x_pow_4 = x ** 4\n",
        "        x_pow_3 = x ** 3\n",
        "        x_pow_2 = x ** 2\n",
        "        x_pow_1 = x\n",
        "\n",
        "        a_s += x_pow_5 * (y_bar - y)\n",
        "        b_s += x_pow_4 * (y_bar - y)\n",
        "        c_s += x_pow_3 * (y_bar - y)\n",
        "        d_s += x_pow_2 * (y_bar - y)\n",
        "        e_s += x_pow_1 * (y_bar - y)\n",
        "        f_s += (y_bar - y)\n",
        "\n",
        "    n = len(outputs_y)\n",
        "\n",
        "    gradient_a = (2 / n) * a_s\n",
        "    gradient_b = (2 / n) * b_s\n",
        "    gradient_c = (2 / n) * c_s\n",
        "    gradient_d = (2 / n) * d_s\n",
        "    gradient_e = (2 / n) * e_s\n",
        "    gradient_f = (2 / n) * f_s\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c, gradient_d, gradient_e, gradient_f)\n",
        "\n",
        "def gradient_ascent_5th_degree(epochs, lr, coeffs):\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_5th_poly(rand_coeffs_to_test, hundred_xs, ys)\n",
        "        rand_coeffs_to_test = tuple(c + lr * g for c, g in zip(rand_coeffs_to_test, loss))\n",
        "        losses.append(loss[0])\n",
        "\n",
        "    print(f\"Losses for coefficients {coeffs}: {losses}\")\n",
        "\n",
        "    return loss[0], rand_coeffs_to_test, [eval_5th_degree(rand_coeffs_to_test, x) for x in hundred_xs]\n",
        "\n",
        "# Generate random x values\n",
        "hundred_xs = np.random.uniform(-1, 1, 100)  # Normalizing the x values\n",
        "\n",
        "# Coefficients for a degree 5 polynomial with 5 roots\n",
        "degree_5_coeffs = [1, -4, 4, -4, 1, 0]\n",
        "\n",
        "# Random initialization for coefficients\n",
        "rand_coeffs_5_init_1 = tuple(random.uniform(-1, 1) for _ in range(6))  # Normalizing the initial coefficients\n",
        "rand_coeffs_5_init_2 = tuple(random.uniform(-1, 1) for _ in range(6))\n",
        "\n",
        "# Perform gradient ascent with different initializations\n",
        "GA_degree_5_rand_init_1 = gradient_ascent_5th_degree(30000, 0.0001, rand_coeffs_5_init_1)\n",
        "GA_degree_5_rand_init_2 = gradient_ascent_5th_degree(30000, 0.0001, rand_coeffs_5_init_2)\n",
        "GA_degree_5_fixed_init = gradient_ascent_5th_degree(30000, 0.0001, degree_5_coeffs)\n",
        "\n",
        "# Plot the results for different initializations\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hundred_xs, GA_degree_5_rand_init_1[2], 'b.', label='Random Initialization 1')\n",
        "plt.plot(hundred_xs, GA_degree_5_rand_init_2[2], 'r.', label='Random Initialization 2')\n",
        "plt.plot(hundred_xs, GA_degree_5_fixed_init[2], 'g.', label='Fixed Initialization')\n",
        "plt.title('Degree 5 Polynomial - Convergence with Different Initializations (Maximization)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tiSOHcGTnWSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}